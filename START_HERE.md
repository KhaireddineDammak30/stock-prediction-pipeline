# ğŸš€ START HERE - Complete Guide to Run Your Pipeline

## ğŸ“‹ What You Need

- âœ… Docker installed on your laptop
- âœ… Docker Compose installed
- âœ… Terminal/Command Prompt
- âœ… Web browser
- âœ… At least 8GB RAM available

## ğŸ¯ Quick Start (5 Minutes)

### Step 1: Open Terminal

**On Mac/Linux:**
- Open Terminal app

**On Windows:**
- Open PowerShell or Command Prompt
- Or use WSL (Windows Subsystem for Linux)

### Step 2: Go to Your Project

```bash
cd /home/khaireddine/projects/stock-prediction-pipeline
```

### Step 3: Start Everything (ONE COMMAND!)

```bash
./scripts/manage.sh start
```

**What this does:**
- âœ… Starts all services (Kafka, Spark, Cassandra, etc.)
- âœ… Waits for services to be ready
- âœ… Initializes databases and topics
- âœ… Shows you status and URLs

**Wait for:** You'll see "âœ… Pipeline startup complete! ğŸ‰"

### Step 4: Open Your Dashboard

Open your web browser and go to:
```
http://localhost:8501
```

**You should see:** The Stock Prediction Dashboard! ğŸ“Š

---

## ğŸ“Š Viewing Your Pipeline

### Main Dashboard (What You'll See)

1. **Stock Symbol Selector** - Choose a stock (AAPL, MSFT, etc.)
2. **Key Metrics** - Total predictions, latest price, average
3. **Price Chart** - Beautiful graph showing predictions over time
4. **Statistics** - Mean, median, standard deviation
5. **Recent Predictions Table** - Latest data

### Other Views You Can Access

| Service | URL | What You'll See |
|---------|-----|-----------------|
| **Main Dashboard** | http://localhost:8501 | Stock predictions and charts |
| **Spark UI** | http://localhost:8080 | Spark cluster status and jobs |
| **Prometheus** | http://localhost:9090 | Metrics and monitoring |
| **Grafana** | http://localhost:3000 | Advanced dashboards (admin/admin) |

---

## ğŸ”‘ Key Points of Your Work

### What Your Pipeline Does

```
1. Producer â†’ Generates stock data
   â†“
2. Kafka â†’ Stores messages
   â†“
3. Spark Streaming â†’ Processes data in real-time
   â†“
4. Cassandra â†’ Stores features and predictions
   â†“
5. Dashboard â†’ Shows you everything!
```

### Key Features

âœ… **Real-time Processing** - Data flows continuously  
âœ… **Machine Learning** - Predicts stock prices  
âœ… **Professional Dashboard** - Beautiful visualizations  
âœ… **Monitoring** - Track everything with Prometheus/Grafana  
âœ… **Scalable** - Can handle lots of data  

---

## ğŸ› ï¸ Complete Step-by-Step Guide

### Part 1: Starting the Pipeline

#### 1.1 Check Docker is Running

```bash
docker ps
```

**Expected:** Should show running containers or empty list (both OK)

#### 1.2 Navigate to Project

```bash
cd /home/khaireddine/projects/stock-prediction-pipeline
```

#### 1.3 Make Scripts Executable (First Time Only)

```bash
chmod +x scripts/*.sh
chmod +x QUICK_TEST.sh
```

#### 1.4 Start Everything

```bash
./scripts/manage.sh start
```

**Watch for:**
- Services starting...
- Waiting for Cassandra... (takes 30-60 seconds)
- âœ… Services ready
- âœ… Infrastructure initialized
- âœ… Pipeline startup complete!

**Time:** About 2-3 minutes total

#### 1.5 Verify Everything Started

```bash
./scripts/manage.sh status
```

**Expected:** All services show "Up" status

---

### Part 2: Starting Data Processing

#### 2.1 Start Spark Streaming Job

**Open a NEW terminal window** (keep the first one open)

```bash
cd /home/khaireddine/projects/stock-prediction-pipeline
./scripts/manage.sh start-streaming
```

**What you'll see:**
- Spark job starting
- Processing messages from Kafka
- Writing to Cassandra
- Batch processing logs

**Keep this running!** (Press Ctrl+C to stop later)

#### 2.2 Wait for Data Collection

**Wait 5-10 minutes** for data to accumulate

**Check progress:**
- Open http://localhost:8080 (Spark UI)
- Look for "streaming_ingest" application
- Check "Streaming" tab for processed records

---

### Part 3: Generating Predictions

#### 3.1 Start Batch Training Job

**Open ANOTHER terminal window**

```bash
cd /home/khaireddine/projects/stock-prediction-pipeline
./scripts/manage.sh start-batch
```

**What this does:**
- Trains ML model on collected data
- Generates predictions
- Saves to Cassandra

**Wait for:** "RMSE: X.XXXX" and "Wrote predictions to Cassandra"

---

### Part 4: Viewing Results

#### 4.1 Open Dashboard

Open browser: **http://localhost:8501**

#### 4.2 Select a Symbol

- Use dropdown or type: **AAPL**
- Click or press Enter

#### 4.3 View Predictions

You'll see:
- ğŸ“Š Price chart
- ğŸ“ˆ Statistics
- ğŸ“‹ Recent predictions table

---

## ğŸ¯ Key Points to Explore

### 1. Dashboard Features

**Try these:**
- Select different symbols (AAPL, MSFT, GOOGL)
- Click "ğŸ”„ Refresh Data" button
- Check sidebar for pipeline status
- View statistics panel

### 2. Spark UI

**Go to:** http://localhost:8080

**See:**
- Active applications
- Worker status
- Resource usage
- Job history

### 3. Monitoring

**Prometheus:** http://localhost:9090
- Query metrics
- Check targets
- View graphs

**Grafana:** http://localhost:3000
- Login: admin/admin
- View dashboards
- Create custom visualizations

---

## ğŸ“Š Understanding What's Happening

### Data Flow

```
Producer (generates data)
    â†“
Kafka (stores messages)
    â†“
Spark Streaming (processes in real-time)
    â†“
HDFS (stores raw data)
    â†“
Cassandra (stores features)
    â†“
Spark Batch (trains model)
    â†“
Cassandra (stores predictions)
    â†“
Dashboard (shows you everything!)
```

### What Each Service Does

| Service | Purpose |
|---------|---------|
| **Producer** | Creates synthetic stock data |
| **Kafka** | Message queue for data |
| **Spark Master** | Coordinates Spark jobs |
| **Spark Workers** | Process data |
| **Cassandra** | Database for features & predictions |
| **HDFS** | File storage for raw data |
| **Streamlit** | Web dashboard |
| **Prometheus** | Metrics collection |
| **Grafana** | Metrics visualization |

---

## ğŸ§ª Testing Your Setup

### Quick Health Check

```bash
./QUICK_TEST.sh
```

**Checks:**
- âœ… All services running
- âœ… Endpoints accessible
- âœ… Infrastructure ready

### Manual Checks

#### Check Producer is Sending Data

```bash
docker logs producer | tail -20
```

**Expected:** See messages like "âœ… Sent tick: ..."

#### Check Kafka Has Messages

```bash
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ticks \
  --max-messages 5
```

**Expected:** See JSON messages with stock data

#### Check Cassandra Has Data

```bash
docker exec -it cassandra cqlsh -e "SELECT COUNT(*) FROM market.predictions;"
```

**Expected:** Number of predictions (0 if batch job hasn't run)

---

## ğŸ›‘ Stopping Everything

### Stop Services

```bash
./scripts/manage.sh stop
```

### Stop and Remove Everything (Deletes Data!)

```bash
./scripts/manage.sh clean
```

**Warning:** This deletes all data!

---

## ğŸ› Troubleshooting

### Problem: Services Won't Start

**Solution:**
```bash
# Check Docker is running
docker ps

# Check logs
docker-compose logs

# Restart
./scripts/manage.sh restart
```

### Problem: Dashboard Shows "No Data"

**Solution:**
1. Make sure streaming job is running
2. Wait 5-10 minutes for data collection
3. Run batch training job
4. Refresh dashboard

### Problem: Can't Connect to Services

**Solution:**
```bash
# Check services are running
docker-compose ps

# Check ports aren't in use
netstat -tulpn | grep 8501
netstat -tulpn | grep 8080

# Restart services
./scripts/manage.sh restart
```

### Problem: Out of Memory

**Solution:**
- Close other applications
- Reduce Docker memory limit
- Or increase system RAM

---

## ğŸ“ Daily Workflow

### Starting Your Day

```bash
# 1. Start pipeline
./scripts/manage.sh start

# 2. Check status
./scripts/manage.sh status

# 3. Start streaming (in separate terminal)
./scripts/manage.sh start-streaming

# 4. Open dashboard
open http://localhost:8501
```

### During Work

- Monitor dashboard: http://localhost:8501
- Check Spark UI: http://localhost:8080
- View logs: `./scripts/manage.sh logs <service>`

### Ending Your Day

```bash
# Stop everything
./scripts/manage.sh stop
```

---

## ğŸ“ Learning Your Project

### Key Files to Understand

1. **`scripts/manage.sh`** - How to control everything
2. **`services/streamlit/app.py`** - Dashboard code
3. **`services/producer/producer.py`** - Data generation
4. **`services/spark/jobs/streaming_ingest.py`** - Real-time processing
5. **`services/spark/jobs/batch_train_predict.py`** - ML training

### Key Concepts

- **Streaming** - Real-time data processing
- **Batch** - Periodic model training
- **Features** - Technical indicators (SMA-5, SMA-20)
- **Predictions** - ML model outputs

---

## âœ… Success Checklist

After following this guide, you should have:

- [ ] All services running
- [ ] Dashboard accessible at http://localhost:8501
- [ ] Spark streaming job running
- [ ] Data flowing through pipeline
- [ ] Predictions visible in dashboard
- [ ] Monitoring accessible (Prometheus, Grafana)

---

## ğŸ‰ Congratulations!

You now know how to:
- âœ… Start your pipeline
- âœ… View the dashboard
- âœ… Understand what's happening
- âœ… Troubleshoot issues

**Your pipeline is running!** ğŸš€

---

## ğŸ“š Next Steps

1. **Explore Dashboard** - Try different symbols, check features
2. **Check Monitoring** - See metrics in Prometheus/Grafana
3. **Read Documentation** - Learn more in other guides
4. **Customize** - Modify configurations, add features

**Happy exploring!** ğŸ“ˆ

